{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import sqlalchemy\n",
    "\n",
    "# TEMP, REMOVE AFTER INDEX FIX\n",
    "import datetime as dt\n",
    "\n",
    "# import Objectiv buh_tuh\n",
    "from buhtuh.pandasql import BuhTuhDataFrame\n",
    "sys.path.extend([\n",
    "    '../../buhtuh',\n",
    "    '../'\n",
    "])\n",
    "\n",
    "from objectiv_buhtuh.util import duplo_basic_features\n",
    "\n",
    "# enable these once we visualize sankey charts\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b47e4-e1c6-443f-a164-31078ed3ae8b",
   "metadata": {},
   "source": [
    "## Get website production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get some data, add database and credentials here\n",
    "engine = sqlalchemy.create_engine('postgresql://objectiv:@localhost:5432/objectiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "## production website data (from sessionized_data + features)\n",
    "basic_features = duplo_basic_features()\n",
    "buh_tuh = BuhTuhDataFrame.from_model(engine=engine, model=basic_features, index=['event_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a13bf-52e0-4082-a4a2-c382871b1713",
   "metadata": {},
   "source": [
    "## Set the timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177f1fb-c741-4269-8086-dbab942afbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the timeframe for analysis\n",
    "selector = (buh_tuh['moment'] >= datetime.date(2021,6,1)) & (buh_tuh['moment'] < datetime.date(2021,10,4))\n",
    "\n",
    "# create one sampled df with timeframe applied and one with the full dataset\n",
    "timeframe_df = buh_tuh[selector]\n",
    "full_df = buh_tuh\n",
    "\n",
    "timeframe_df.sort_values(by='moment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d4954-cabc-44e7-81b8-d18f9d303ed9",
   "metadata": {},
   "source": [
    "## Set the time aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a697920-a443-4593-8acd-71c514aa7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose for which level of time aggregation the rest of the analysis will run\n",
    "# supports all Postgres datetime template patterns:\n",
    "# https://www.postgresql.org/docs/9.1/functions-formatting.html#FUNCTIONS-FORMATTING-DATETIME-TABLE\n",
    "\n",
    "agg_level = 'YYYYIW'\n",
    "\n",
    "def time_aggr(bt, format, aggr):\n",
    "    bt[format.lower()] = bt['moment'].format(format)\n",
    "    group = bt.groupby([format.lower()])\n",
    "    return group[list(aggr.keys())].aggregate(aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b453c8-db28-4f74-be09-0664d32b3e22",
   "metadata": {},
   "source": [
    "## User & session totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a4b0c-49a3-44cc-a4b1-260a7f818c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total users & sessions, we can resuse these later\n",
    "total_users = timeframe_df.groupby()['user_id'].nunique()\n",
    "total_sessions = timeframe_df.groupby()['session_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697e0d3-8443-427b-a3a9-09342adf624e",
   "metadata": {},
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e00ffd-9940-4ff3-b5db-2734730f240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unique users \n",
    "users = time_aggr(timeframe_df, agg_level, {'user_id':'nunique'})\n",
    "\n",
    "users.sort_values(by=agg_level.lower(), ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d9b1e-5159-416d-a904-75b63b795e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize users\n",
    "users.sort_values(by=agg_level.lower(), ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f5454-f7c7-409b-a6d0-996e078c851a",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3c2a3-9616-43d6-b529-acf5f6d210d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unique sessions\n",
    "sessions = time_aggr(timeframe_df, agg_level, {'session_id':'nunique'})\n",
    "\n",
    "sessions.sort_values(by=agg_level.lower(), ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fc334-17c9-4604-85dd-24fa43abaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize sessions\n",
    "sessions.sort_values(by=agg_level.lower(), ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888435d-4287-4cdc-8767-985fe2b9c216",
   "metadata": {},
   "source": [
    "## Sessions per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77706040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge users and sessions\n",
    "users_sessions = sessions.merge(users, how='inner', on=agg_level.lower())\n",
    "\n",
    "# calculate average sessions per user\n",
    "users_sessions['sessions_per_user_avg'] = users_sessions['session_id_nunique'] / users_sessions['user_id_nunique']\n",
    "\n",
    "# clean-up columns\n",
    "del(users_sessions['session_id_nunique'])\n",
    "del(users_sessions['user_id_nunique'])\n",
    "\n",
    "users_sessions.sort_values(by=agg_level.lower(), ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d52d8-08df-4133-a2da-6cedd81f22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize average sessions per user\n",
    "users_sessions.sort_values(by=agg_level.lower(), ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbaaf82-ee41-4602-a2cd-f622fa84b0b2",
   "metadata": {},
   "source": [
    "## New users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917e1a3-5b16-4365-98e0-ea0040babebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define first seen per user, based on full dataset\n",
    "user_first_seen = full_df.groupby(['user_id'])['moment'].min()\n",
    "user_first_seen[agg_level.lower()] = user_first_seen['moment_min'].format(agg_level)\n",
    "\n",
    "# calculate new users for each timeframe\n",
    "new_users = user_first_seen.groupby(agg_level.lower())['user_id'].nunique()\n",
    "\n",
    "# merge with total users, to calculate ratio and limit to timerange\n",
    "new_total_users = users.merge(new_users, how='inner', on=agg_level.lower())\n",
    "\n",
    "# rename and clean-up columns\n",
    "new_total_users['total_users'] = new_total_users['user_id_nunique_x']\n",
    "new_total_users['new_users'] = new_total_users['user_id_nunique_y']\n",
    "del(new_total_users['user_id_nunique_x'])\n",
    "del(new_total_users['user_id_nunique_y'])\n",
    "\n",
    "# calculate new & returning user share\n",
    "new_total_users['new_user_share'] = new_total_users['new_users'] / new_total_users['total_users']\n",
    "new_total_users['returning_user_share'] = (new_total_users['total_users'] - new_total_users['new_users']) / new_total_users['total_users']\n",
    "\n",
    "new_total_users.sort_values(by=agg_level.lower(), ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9d062-dd61-49c2-b669-bf0f45c68451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize new users\n",
    "new_total_users[['new_users', 'total_users']].sort_values(by=agg_level.lower(), ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be740b11-d552-4501-b8b3-48cb077f8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize returning users\n",
    "new_total_users[['returning_user_share']].sort_values(by=agg_level.lower(), ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2a479-bd03-4cba-be97-83ccfbf38337",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8fc165-5b6a-4df4-839e-53b6df4084f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total sessions per user\n",
    "total_sessions_user = timeframe_df.groupby(['user_id'])['session_id'].nunique()\n",
    "\n",
    "# calculate frequency\n",
    "frequency = total_sessions_user.groupby(['session_id_nunique'])['user_id'].nunique()\n",
    "\n",
    "# add total users and calculate share per number of sessions\n",
    "frequency['share_of_users'] = frequency['user_id_nunique'] / total_users['user_id_nunique'][1]\n",
    "\n",
    "frequency.sort_values(by='session_id_nunique', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771461f0-9cc6-482e-ada3-76a4da4152de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize frequency\n",
    "frequency[['share_of_users']].sort_values(by='session_id_nunique', ascending=True).head(10).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046eb7e6-fba5-4048-ba17-8cff59d92b38",
   "metadata": {},
   "source": [
    "## Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c743b4-dae4-41b9-a4d5-15a87c3b295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total user per feature\n",
    "# users_per_event = timeframe_df.groupby(['feature'])['user_id'].nunique()\n",
    "\n",
    "events_users = timeframe_df[['moment', 'feature', 'user_id']]\n",
    "events_users[agg_level.lower()] = events_users['moment'].format(agg_level)\n",
    "\n",
    "# calculate hits per session\n",
    "users_per_event = events_users.groupby([agg_level.lower(), 'feature'])['user_id'].nunique()\n",
    "\n",
    "users_per_event.sort_values(by=[agg_level.lower(), 'user_id_nunique'], ascending=False).head()\n",
    "\n",
    "# TODO: \n",
    "# 1) add feature aggregation magic here, so we make the features show-off what we can do much more \n",
    "# 2) add location stack, showing the power of this very soon in the demo's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf9cf5-c4e6-4d71-a15d-64bea1bf4d55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96311ded-e799-4a27-a466-2c8dcce5ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# We can do much better here once we integrate feature selection & aggregation\n",
    "\n",
    "# NOTE: WE NEED TO UPDATE THIS ONCE THE FIRST NEW EVENT FORMAT DATA COMES IN FOR A CONVERSION\n",
    "# set the goal event that you define as conversion, using our subcribe-to-mailing\n",
    "conv_selector = (timeframe_df['feature'] == '(WebDocumentContext,#document),(InputContext,keep-me-posted-input),(ButtonContext,subscribe)')\n",
    "\n",
    "# create df with only conversion events\n",
    "conversions_df = timeframe_df[conv_selector]\n",
    "\n",
    "# calculate conversions, now per user, but can easily be aggregated to session_id instead\n",
    "conversions = time_aggr(conversions_df, agg_level, {'user_id':'nunique'})\n",
    "\n",
    "# merge with users, but can easily be done with sessions instead\n",
    "conversion_rate = conversions.merge(users)\n",
    "conversion_rate = conversions.merge(users, how='inner', on=agg_level.lower())\n",
    "\n",
    "# clean-up and rename columns\n",
    "conversion_rate['converting_users'] = conversion_rate['user_id_nunique_x']\n",
    "conversion_rate['total_users'] = conversion_rate['user_id_nunique_y']\n",
    "del(conversion_rate['user_id_nunique_x'])\n",
    "del(conversion_rate['user_id_nunique_y'])\n",
    "\n",
    "# calculate conversion rate\n",
    "conversion_rate['conversion_rate'] = conversion_rate['converting_users'] / conversion_rate['total_users']\n",
    "\n",
    "conversion_rate.sort_values(by=agg_level.lower(), ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91715bec-e0c2-4abc-b049-0b7b590348ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize conversion rate\n",
    "conversion_rate[['conversion_rate']].sort_values(by=agg_level.lower(), ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdb781-d188-4f7c-b85d-d96262ff7e69",
   "metadata": {},
   "source": [
    "## Bounce rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81a91-e85c-4f96-b534-dd39fdacf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we need to limit this to page or screen views, instead of all events. Do this once we have integration feature selection.\n",
    "\n",
    "# gather sessions, hits per timeframe\n",
    "hits_sessions = timeframe_df[['moment', 'session_id', 'session_hit_number']]\n",
    "hits_sessions[agg_level.lower()] = hits_sessions['moment'].format(agg_level)\n",
    "\n",
    "# calculate hits per session\n",
    "hits_per_session = hits_sessions.groupby([agg_level.lower(), 'session_id'])['session_hit_number'].nunique()\n",
    "\n",
    "# select sessions with only one hit\n",
    "hit_selector = (hits_per_session['session_hit_number_nunique'] == 1)\n",
    "single_hit_sessions = hits_per_session[hit_selector].to_frame()\n",
    "\n",
    "# count these single hit sessions per timeframe\n",
    "bounced_sessions = single_hit_sessions.groupby([agg_level.lower()])['session_id'].nunique()\n",
    "\n",
    "# merge with total sessions and calculate bounce rate\n",
    "bounce_rate = bounced_sessions.merge(sessions, how='inner', on=agg_level.lower())\n",
    "\n",
    "bounce_rate['bounce_rate'] = bounce_rate['session_id_nunique_x'] / bounce_rate['session_id_nunique_y']\n",
    "del(bounce_rate['session_id_nunique_x'])\n",
    "del(bounce_rate['session_id_nunique_y'])\n",
    "\n",
    "bounce_rate.sort_values(by=agg_level.lower(), ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d126de4-cb9d-4f94-8d54-4074721385db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize bounce rate\n",
    "bounce_rate[['bounce_rate']].sort_values(by=agg_level.lower(), ascending=True).head(60).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c06f91-3fe1-4e25-8706-601e1b223393",
   "metadata": {},
   "source": [
    "## Session duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279e793-e5a4-4a4b-968a-db8c5949f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate duration of each session\n",
    "session_duration = timeframe_df.groupby(['session_id']).aggregate(['moment','moment'],['min','max'])\n",
    "session_duration['session_duration'] = session_duration['moment_max'] - session_duration['moment_min']\n",
    "\n",
    "# check which sessions have duration of zero\n",
    "session_duration['session_duration_zero'] = session_duration['session_duration'] == '0'\n",
    "\n",
    "# adding time aggregation, so we can group on this\n",
    "session_duration[agg_level.lower()] = session_duration['moment_min'].format(agg_level)\n",
    "\n",
    "# calculate average session duration\n",
    "avg_session_duration = session_duration.groupby([agg_level.lower(), 'session_duration_zero'])['session_duration'].average()\n",
    "\n",
    "# count the number of sessions with duration zero or more\n",
    "session_counts = session_duration.groupby([agg_level.lower(), 'session_duration_zero'])['session_id'].count()\n",
    "\n",
    "# merge avg session duration and counts\n",
    "avg_duration_counts = avg_session_duration.merge(session_counts, how='inner', on=[(agg_level.lower()),('session_duration_zero')])\n",
    "\n",
    "# merge with total sessions and calculate share\n",
    "duration_breakdown = avg_duration_counts.merge(sessions, how='inner', on=agg_level.lower())\n",
    "\n",
    "duration_breakdown['share_of_sessions'] = duration_breakdown['session_id_count'] / duration_breakdown['session_id_nunique']\n",
    "del(duration_breakdown['session_id_nunique'])\n",
    "\n",
    "duration_breakdown.sort_values(by=agg_level.lower(), ascending=False).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0f2c5-8e20-4b54-bdfa-dcb920794e5d",
   "metadata": {},
   "source": [
    "## WIP Session duration between specific events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb0024-91db-4a8b-a045-21934c117020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the start and stop events to measure the duration\n",
    "start_event = '(WebDocumentContext,#document)'\n",
    "stop_event = '(WebDocumentContext,#document),(SectionContext,footer)'\n",
    "\n",
    "start_event_selector = (timeframe_df['feature'] == start_event)\n",
    "stop_event_selector = (timeframe_df['feature'] == stop_event)\n",
    "\n",
    "# create df filtered on these events\n",
    "start_event_df = timeframe_df[start_event_selector]\n",
    "stop_event_df = timeframe_df[stop_event_selector]\n",
    "\n",
    "# select only the columns needed\n",
    "start_event_df = start_event_df[['moment', 'session_id']]\n",
    "stop_event_df = stop_event_df[['moment', 'session_id']]\n",
    "\n",
    "# merge based on session_id\n",
    "start_stop_moments = start_event_df.merge(stop_event_df, how='inner', on='session_id')\n",
    "\n",
    "# clean-up and rename columns\n",
    "start_stop_moments['moment_start'] = start_stop_moments['moment_x']\n",
    "start_stop_moments['moment_stop'] = start_stop_moments['moment_y']\n",
    "del(start_stop_moments['moment_x'])\n",
    "del(start_stop_moments['moment_y'])\n",
    "\n",
    "# calculate diff between start & stop, to use later for filtering to real duration\n",
    "start_stop_moments['moment_diff'] = start_stop_moments['moment_stop'] - start_stop_moments['moment_start']\n",
    "\n",
    "# diff can not be negative\n",
    "negative_diff_selector = (start_stop_moments['moment_stop'] >= start_stop_moments['moment_start'])\n",
    "start_stop_filtered = start_stop_moments[negative_diff_selector]\n",
    "\n",
    "# for each stop event, select the closest preceeding start event\n",
    "start_stop_diff = start_stop_filtered.groupby(['session_id', 'moment_stop'])['moment_diff'].min()\n",
    "\n",
    "# BELOW DOES NOT WORK YET UNTIL WE HAVE A PLAN FOR INDEXES\n",
    "# start_stop_diff[agg_level.lower()] = start_stop_diff['moment_stop'].format(agg_level)\n",
    "\n",
    "# TEMP UGLY WORKAROUND UNTIL WE HAVE INDEX PLAN\n",
    "temp_fix = start_stop_diff.head(100000)\n",
    "temp_fix_index = temp_fix.reset_index()\n",
    "\n",
    "# adding time aggregation, based on stop event, so we can group on this\n",
    "temp_fix_index['date'] = temp_fix_index['moment_stop'].dt.date\n",
    "\n",
    "# calculate duration between start & stop events\n",
    "start_stop_duration = temp_fix_index.groupby('date').agg({'moment_diff_min':'sum'})\n",
    "start_stop_duration.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f25752-6cbe-4119-ac7e-79d4c095e793",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a504f06-ce9b-44d0-b1e1-bc4047a51843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below parts first require some next steps in dub_buh_tuh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84339df-e25a-43ff-a527-8ff7aa674128",
   "metadata": {},
   "source": [
    "## Conversion funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ea3f2-a2b0-4701-80fe-3cce1e54b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Self-merge is giving not the ouput we expect. \n",
    "# Without that, we can not create a sankey that looks like a familiar funnel. \n",
    "# See example here https://gitlab.com/newrelity/objectiv-taxonomy-prototypes/-/blob/web-analytics/data-science/issue_example_self_merge.ipynb\n",
    "\n",
    "# showing the sequence of events for converting users\n",
    "\n",
    "# resuse the df with only conversion events, select the users and their conversion moment\n",
    "converting_users = conversions_df['user_id', 'moment']\n",
    "\n",
    "# for now, we focus on the first conversion event. Later it is nice to also make it possible to see events between first and 2nd conversion, and so on.\n",
    "converting_users = converting_users.groupby(['user_id'])['moment'].min()\n",
    "converting_users['first_conversion_moment'] = converting_users['moment_min']\n",
    "del(converting_users['moment_min'])\n",
    "\n",
    "# merge with the df that has all user events in the timeframe\n",
    "converting_users_events = timeframe_df.merge(converting_users, [('user_id', 'user_id')])\n",
    "\n",
    "# select all events that converting users had up to their first conversion moment\n",
    "event_selector = (converting_users_events['moment'] <= converting_users_events['first_conversion_moment'])\n",
    "pre_conversion_events = converting_users_events[event_selector]\n",
    "\n",
    "# create pairs of from-to events based on session hit number\n",
    "event_sequence = pre_conversion_events['session_id', 'session_hit_number', 'feature']\n",
    "\n",
    "event_pairs = event_sequence.merge(event_sequence, [('session_id')])\n",
    "\n",
    "event_pairs.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ae37e-d3a9-4ff4-a812-58344f07c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sank = pd.read_csv('buh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7547db-3fad-412c-b566-01f5582dccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = set(df_sank['source']).union(set(df_sank['target']))\n",
    "df_sank['source'] = pd.Categorical(df_sank['source'], categories=categories)\n",
    "df_sank['target'] = pd.Categorical(df_sank['target'], categories=categories)\n",
    "\n",
    "text_in_title = str('title')\n",
    "node = dict(\n",
    "      pad=15,\n",
    "      thickness=20,\n",
    "      line=dict(color=\"black\", width=0.5),\n",
    "      label=df_sank.source.cat.categories,\n",
    "      color='blue'\n",
    "    )\n",
    "link = pd.concat([df_sank[['source', 'target']].apply(lambda x: x.cat.codes), df_sank['value']], axis=1).to_dict('list')\n",
    "fig = go.Figure(go.Sankey(arrangement=\"fixed\", link=link, node=node), {'clickmode': 'event+select'})\n",
    "fig.update_layout(title_text=text_in_title, font_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c66a2-97ee-4888-83d2-122547bf53c6",
   "metadata": {},
   "source": [
    "## User timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13effa-fbc1-42fb-8faf-85c40948602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the timeline of an indivual user's events\n",
    "# NOTE: we can make this better with feature selection & aggregation\n",
    "\n",
    "# select the spefic user we want to replay\n",
    "user_id_selector = (buh_tuh['user_id'] == '320db8ee-847c-424b-8291-c65d021575aa')\n",
    "\n",
    "# create df with only this user's events\n",
    "# NOTE: timeframe_df['user_id_selector'] breaks: \"# We only support first level boolean indices for now\", so doing on full df for now\n",
    "selected_user_df = buh_tuh[user_id_selector]\n",
    "\n",
    "# left join conversions df, so we can check if the user converted\n",
    "user_timeline = selected_user_df.merge(conversions_df,how='left')\n",
    "\n",
    "# rename and clean-up columns\n",
    "user_timeline['moment'] = user_timeline['moment_left'] \n",
    "user_timeline['feature'] = user_timeline['feature_left']\n",
    "user_timeline['conversion_feature'] = user_timeline['feature_right']\n",
    "\n",
    "# show relevant columns\n",
    "user_timeline['moment','feature','conversion_feature'].sort_values({'moment':True}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e283e6-cc77-412d-91ec-9915fcbc7764",
   "metadata": {},
   "source": [
    "## Retention cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9547912-5207-475d-99cc-345e82e36c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# continue on this when we have datetime intervals, so we can calculate start & end moments of cohorts.\n",
    "\n",
    "# get the time aggregations where there are users\n",
    "timeframes = users\n",
    "timeframes = timeframes.head(100).reset_index()\n",
    "\n",
    "# cleanup columns we don't need\n",
    "del(timeframes['user_id_nunique'])\n",
    "del(timeframes['share_of_total'])\n",
    "\n",
    "# reset index and use that as cohort numbering\n",
    "timeframes2 = timeframes.rename_axis('cohort_nr').reset_index()\n",
    "timeframes2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec41a8-09bc-4922-b12f-83dd1725bf67",
   "metadata": {},
   "source": [
    "## Events flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f06c4-ac0f-4c6b-889b-918cba8be266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events per session hit number\n",
    "events_per_hit_number = buh_tuh[selector].groupby(['session_hit_number', 'feature'])['session_id'].nunique()\n",
    "\n",
    "events_per_hit_number.sort_values({'session_hit_number':True}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95769feb-fc26-4aa1-9e12-205aa3ed7fc8",
   "metadata": {},
   "source": [
    "## Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3fef3-e4d7-4adf-a8c3-ca4c4d789606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pick this up once we have window functions\n",
    "\n",
    "# \"the number of days between the close of one session and the opening of another\"\n",
    "test = timeframe_df.groupby(['user_id', 'session_id']).aggregate(['moment','moment'],['min','max'])\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589de70-d5d2-4b1b-8529-ed12c0d3e898",
   "metadata": {},
   "source": [
    "## Traffic source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42d550-625b-451f-8cb4-fed192483469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# For Traffic Source, Geo and Device metrics, we would need to get source/geo/device data from GlobalContext in a easy way.\n",
    "# We can then also blend it in all metrics above as slicing option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56cb88f-b2df-44e2-80ba-c1c384978689",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Geo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a12b72-3872-488c-934d-eed3f262560e",
   "metadata": {},
   "source": [
    "## Devices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
