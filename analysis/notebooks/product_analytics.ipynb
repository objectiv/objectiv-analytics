{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814152bb-a253-4003-a954-37d7e2677510",
   "metadata": {},
   "source": [
    "# Objectiv example notebook\n",
    "\n",
    "This demo notebook enables you to play with Bach, our modeling library, to get an idea of what it can do. A live web version is also available [here](https://notebook.objectiv.io/lab?path=product_analytics.ipynb).\n",
    "\n",
    "A few notes about this example notebook:\n",
    "* It uses a real dataset from objectiv.io, collected with an unaltered version of Objectiv’s [tracker](https://objectiv.io/docs/tracking/). No cleaning or transformation* has been applied to the data. Objectiv’s tracker uses the [open taxonomy for analytics](https://objectiv.io/docs/taxonomy/) to collect clean data that’s ready to model on.\n",
    "*  You can also generate your own events and use/see them in this notebook. Check out our [Quickstart Guide](https://www.objectiv.io/docs/quickstart-guide) for instructions.\n",
    "* It is connected to a PostgreSQL database and runs directly on the full dataset. You can use Pandas-like dataframe operations, that Bach translates to SQL under the hood.\n",
    "* This notebook demonstrates only a selection all of the operations that are supported by Bach. Check out the [docs](https://objectiv.io/docs/modeling/reference#api-reference) for the full rundown.\n",
    "* You can also use this notebook for your own website or app once you've instrumented it with Objectiv's tracker.\n",
    "\n",
    "For any question, please join our [Slack channel](https://join.slack.com/t/objectiv-io/shared_invite/zt-u6xma89w-DLDvOB7pQer5QUs5B_~5pg).\n",
    "\n",
    "<sub>*for privacy reasons, IPs have been removed and timeframes have been cut from the initial dataset.</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import sqlalchemy\n",
    "import os\n",
    "\n",
    "from jupyter_dash import JupyterDash as Dash\n",
    "\n",
    "# import Objectiv Bach\n",
    "from bach import DataFrame\n",
    "from bach_open_taxonomy import FeatureFrame, basic_feature_model\n",
    "from bach_open_taxonomy.sankey_dash import get_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b47e4-e1c6-443f-a164-31078ed3ae8b",
   "metadata": {},
   "source": [
    "## Connect to full dataset in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to full postgresql dataset, add database and credentials here\n",
    "dsn = os.environ.get('DSN', 'postgresql://objectiv:@localhost:5432/objectiv')\n",
    "engine = sqlalchemy.create_engine(dsn, pool_size=1, max_overflow=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Bach dataframe based on the full dataset\n",
    "# note that the database is not queried for operations on this dataframe. The database will only be queried\n",
    "# when data is outputted to the python environment (ie when using .head() or .to_pandas()).\n",
    "basic_features = basic_feature_model()\n",
    "df = DataFrame.from_model(engine=engine, model=basic_features, index=['event_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab9da6-03b0-4dd1-8d69-6cabb5e930d3",
   "metadata": {},
   "source": [
    "## Set sample / unsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7a4cb-9453-4009-8ba3-cea3f86e9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if desired, sample the data to develop models, for demo purposes we skip the sampling and work on full set\n",
    "# all underlying data for df gets queried once in order to create the sample.\n",
    "\n",
    "# df = df.get_sample(table_name='basic_features_sample', sample_percentage=10, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b95780-2456-4fd6-adaf-280f686244e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is possible to apply all data manipulations on the full data set at any time.\n",
    "# to unsample the data and run all models below on full dataset, use:\n",
    "\n",
    "# df = df.get_unsampled()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfc42c-b58f-4256-ab4a-8cd8ecfaa330",
   "metadata": {},
   "source": [
    "## Add global contexts & location stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b21e05-70d3-4e8c-ac0c-18dda5899d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the global contexts and location stack as custom dtype so we can use them in modeling\n",
    "# global_contexts and location_stack are json type data columns. Setting custom dtypes extends the functionality\n",
    "# for easy access to the contents of these columns.\n",
    "df['global_contexts'] = df.global_contexts.astype('objectiv_global_context')\n",
    "df['location_stack'] = df.location_stack.astype('objectiv_location_stack')\n",
    "\n",
    "# functions specific for columns of the type 'objectiv_global_context' can be accessed using the `gc` name space.\n",
    "# for 'objectiv_location_stack' type columns this is `ls`\n",
    "\n",
    "# add the event location from the location_stack as new column to the df, using ls function:\n",
    "df['event_location'] = df.location_stack.ls.nice_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1cd456-699f-4e92-b225-3b565504683d",
   "metadata": {},
   "source": [
    "## Set the user application(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba425d-ccfd-4c4e-bf18-3a0195d605d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to df with the user application from the global contexts, using gc function\n",
    "df['user_application'] = df.global_contexts.gc.application\n",
    "\n",
    "# select one or more user application(s) for analysis, in this case objectiv.io website and docs\n",
    "df = df[(df['user_application'] == 'objectiv-website') | (df['user_application'] == 'objectiv-docs')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d4954-cabc-44e7-81b8-d18f9d303ed9",
   "metadata": {},
   "source": [
    "## Set the time aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a697920-a443-4593-8acd-71c514aa7406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose for which level of time aggregation the rest of the analysis will run\n",
    "# supports all Postgres datetime template patterns: https://www.postgresql.org/docs/9.1/functions-formatting.html#FUNCTIONS-FORMATTING-DATETIME-TABLE\n",
    "\n",
    "agg_level = 'YYYYMMDD'\n",
    "\n",
    "# add the time aggregation as new column to the dataframes, so we can group on this later\n",
    "df['time_aggregation'] = df['moment'].dt.sql_format(agg_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a13bf-52e0-4082-a4a2-c382871b1713",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set the timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f208792-69e9-4f90-bd6d-db28e5cc28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the timeframe for analysis\n",
    "timeframe_selector = (df['moment'] >= datetime.date(2021,10,21)) \n",
    "\n",
    "# create a new df with timeframe applied \n",
    "timeframe_df = df[timeframe_selector]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c55ade-f0f6-44ba-a54a-af659fc4890f",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177f1fb-c741-4269-8086-dbab942afbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only now the data gets queried. It is therefore recommended to limit the use of functions that query the\n",
    "# database or use a sample when it is not (yet) required to query all data. The documentation of Bach always\n",
    "# indicates in case the database gets queried.\n",
    "timeframe_df.sort_values(by='moment', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697e0d3-8443-427b-a3a9-09342adf624e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e00ffd-9940-4ff3-b5db-2734730f240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unique users per timeframe\n",
    "users = timeframe_df.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# calculate total users, to reuse later\n",
    "total_users = timeframe_df['user_id'].nunique()\n",
    "\n",
    "users.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d9b1e-5159-416d-a904-75b63b795e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize users\n",
    "fig = px.line(data_frame = users.to_pandas())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f5454-f7c7-409b-a6d0-996e078c851a",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3c2a3-9616-43d6-b529-acf5f6d210d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unique sessions\n",
    "sessions = timeframe_df.groupby('time_aggregation').aggregate({'session_id':'nunique'})\n",
    "\n",
    "sessions.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fc334-17c9-4604-85dd-24fa43abaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize sessions\n",
    "fig = px.line(data_frame = sessions.to_pandas())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888435d-4287-4cdc-8767-985fe2b9c216",
   "metadata": {},
   "source": [
    "## Sessions per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77706040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge users and sessions\n",
    "users_sessions = sessions.merge(users, how='inner', on='time_aggregation')\n",
    "\n",
    "# calculate average sessions per user\n",
    "users_sessions['sessions_per_user_avg'] = users_sessions['session_id_nunique'] / users_sessions['user_id_nunique']\n",
    "\n",
    "# clean-up columns\n",
    "users_sessions.drop(columns=['session_id_nunique', 'user_id_nunique'], inplace=True)\n",
    "\n",
    "users_sessions.sort_values('time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d52d8-08df-4133-a2da-6cedd81f22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize average sessions per user\n",
    "fig = px.line(data_frame = users_sessions.to_pandas())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbaaf82-ee41-4602-a2cd-f622fa84b0b2",
   "metadata": {},
   "source": [
    "## New users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927dba0-56d3-4185-b99e-9dd93c3de2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define first seen per user, based on dataset with not timeframe applied\n",
    "user_first_seen = df.groupby('user_id').aggregate({'time_aggregation':'min', 'session_id':'min'})\n",
    "\n",
    "# select all users that have been active in the time\n",
    "active_users = timeframe_df['user_id'].unique()\n",
    "\n",
    "# merge with users that have been active in the timeframe\n",
    "user_first_seen = user_first_seen.merge(active_users, how='inner', on='user_id')\n",
    "\n",
    "# calculate new users for each timeframe\n",
    "new_users = user_first_seen.groupby('time_aggregation_min').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# merge with total users to calculate ratio and limit to timerange\n",
    "new_total_users = users.merge(new_users, how='inner', left_on='time_aggregation', right_on='time_aggregation_min', suffixes=('_total', '_new'))\n",
    "\n",
    "# set time_aggregation as single index\n",
    "new_total_users = new_total_users.set_index('time_aggregation')\n",
    "\n",
    "# calculate new & returning user share\n",
    "new_total_users['new_user_share'] = new_total_users['user_id_nunique_new'] / new_total_users['user_id_nunique_total']\n",
    "new_total_users['returning_user_share'] = (new_total_users['user_id_nunique_total'] - new_total_users['user_id_nunique_new']) / new_total_users['user_id_nunique_total']\n",
    "\n",
    "new_total_users.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9d062-dd61-49c2-b669-bf0f45c68451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize new users\n",
    "fig = px.line(data_frame = new_total_users[['user_id_nunique_new', 'user_id_nunique_total']].to_pandas())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be740b11-d552-4501-b8b3-48cb077f8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize returning users\n",
    "fig = px.line(data_frame = new_total_users[['returning_user_share']].to_pandas())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046eb7e6-fba5-4048-ba17-8cff59d92b38",
   "metadata": {},
   "source": [
    "## Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343aeb0a-c5cf-4d69-9fe1-c25275026d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Objectiv, you can create features that utilize the context of where they occur on the UI, using the location stack\n",
    "# while it is possible to use the event_type and location_stack as is to describe individual features,\n",
    "# the location stack can be leveraged to group and aggregate various features at different levels of location 'depth'\n",
    "# of your product. \n",
    "\n",
    "# choose for which application(s) to create features, in this case we select the Objectiv website\n",
    "feature_creation_df = timeframe_df[(timeframe_df['user_application'] == 'objectiv-website')]\n",
    "\n",
    "# limit the timerange to match the latest taxonomy version applied as example on the website\n",
    "feature_creation_df = feature_creation_df[(feature_creation_df['moment'] >= datetime.date(2021,11,15))]\n",
    "\n",
    "# first, create a feature frame that will be used to create features\n",
    "feature_frame = FeatureFrame.from_data_frame(df=feature_creation_df, location_stack_column='location_stack', event_column='event_type', overwrite=True)\n",
    "feature_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b55de19",
   "metadata": {},
   "source": [
    "**feature creation slicing the location stack**  \n",
    "The `.json[]` syntax of location stacks allows you to slice with integers, but also dictionaries can be passed. If a dictionary matches\n",
    "a context object in the stack, all objects of the stack starting at that object will be returned.  \n",
    "  \n",
    "**An example**  \n",
    "We want to return only location stacks sub sets that contain this object:\n",
    "```javascript\n",
    "{\"id\": \"contributors\", \"_type\": \"SectionContext\"}\n",
    "```\n",
    "This means that if a location stack looks like this:\n",
    "```json\n",
    "[{\"id\": \"#document\", \"_type\": \"WebDocumentContext\"},\n",
    " {\"id\": \"main\", \"_type\": \"SectionContext\"},\n",
    " {\"id\": \"core-team\", \"_type\": \"SectionContext\"},\n",
    " {\"id\": \"contributors\", \"_type\": \"SectionContext\"},\n",
    " {\"id\": \"jansentom\", \"_type\": \"SectionContext\"},\n",
    " {\"id\": \"contributor-card\", \"_type\": \"SectionContext\"}]\n",
    "```\n",
    "The returned location stack looks like this:\n",
    "```json\n",
    "[{\"id\": \"contributors\", \"_type\": \"SectionContext\"},\n",
    " {\"id\": \"jansentom\", \"_type\": \"SectionContext\"},\n",
    " {\"id\": \"contributor-card\", \"_type\": \"SectionContext\"}]\n",
    "```\n",
    "In case a location stack does not contain the object, `None` is returned. The syntax for selecting like this is: \n",
    "```python\n",
    "feature_frame[\"contributors_features\"] = feature_frame.location_stack.json[{\"_type\": \"SectionContext\", \"id\": \"contributors\"}:]\n",
    "```\n",
    "\n",
    "Now we want to create a location stack that only contains the first object of this stack. For example if you are  not interested in clicks on individual contributors, but want to aggregate clicks on all of them. This can be done by using slices:\n",
    "```python\n",
    "feature_frame[\"contributors_aggregated\"] = feature_frame.contributors_features.json[:1]\n",
    "```\n",
    "result:\n",
    "```json\n",
    "[{\"id\": \"contributors\", \"_type\": \"SectionContext\"}]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23885cea",
   "metadata": {},
   "source": [
    "**feature creation with Dash app**  \n",
    "using a Dash app, you can visualize all events with the location stack and create features.\n",
    "\n",
    "the database gets queried for this to get all unique features.\n",
    "\n",
    "as an example, we'll create features:\n",
    "1. the job annoucement bar that is on both Home & About pages  \n",
    "2. conversion, in this case going to GitHub repo\n",
    "3. contributor features  \n",
    "4. aggregate all contributers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features are created\n",
    "feature_frame['announcement_bar_features'] = feature_frame.location_stack.json[{'_type': 'SectionContext', 'id': 'announcement-bar'}:]\n",
    "feature_frame['conversion'] = feature_frame.location_stack.json[{'_type': 'LinkContext', 'id': 'cta-repo-button'}:]\n",
    "feature_frame['contributors_features'] = feature_frame.location_stack.json[{'_type': 'SectionContext', 'id': 'contributors'}:]\n",
    "\n",
    "# this returns the stack of 'contributors_features' up to the first object in the stack (and therefore aggregates all\n",
    "# following objects in the stack)\n",
    "feature_frame['contributors_aggregated'] = feature_frame.contributors_features.json[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb97d2",
   "metadata": {},
   "source": [
    "**Visualizing the stack**  \n",
    "Now we can visualize the location stack. You can select the features with 'Location stack column to visualize'. The width of the links indicates the number of hits (given the selected event type). The number of hits is also the number displayed when hovering over a node.  \n",
    "\n",
    "It is also possible to create features using the tool by clicking nodes, or slicing the selected location stack. Clicking 'Add to Feature Frame' adds the feature to the feature frame.  \n",
    "  \n",
    "Try selecting the just created features. When the event type 'ClickEvent' is selected and switching between 'contributors_features' and 'contributors_aggregated', it shows how the clicks on individual contributors are aggregated.  \n",
    "  \n",
    "By clicking on nodes, or slicing in the sankey tool, Features can also be created. Try recreating the features above starting from the 'location_stack' column as 'Location stack column to visualize'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfe4ee-38a7-43a6-925e-a687fa73820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = get_app(Dash, feature_frame, dash_options={'server_url': 'http://localhost:8053'})\n",
    "app.run_server(mode='inline', height = 1100, port=8053, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f870b-347e-41a3-bd5c-14e8f3933895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are happy with the result, write these creatured features to the working df\n",
    "feature_creation_df = feature_frame.write_to_full_frame()\n",
    "feature_creation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01e44e-e68e-4b84-93e6-b6a405a56a01",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c743b4-dae4-41b9-a4d5-15a87c3b295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features we just created\n",
    "created_features = feature_creation_df[(feature_creation_df.conversion.notnull()) | \n",
    "                                (feature_creation_df.announcement_bar_features.notnull()) |\n",
    "                                (feature_creation_df.contributors_features.notnull()) |\n",
    "                                (feature_creation_df.contributors_aggregated.notnull())]\n",
    "\n",
    "# get the number of total users and hits per feature\n",
    "users_per_event = created_features.groupby(['user_application', 'event_type', 'event_location']).aggregate({'user_id':'nunique','session_hit_number':'count'})\n",
    "\n",
    "users_per_event.sort_values(by=['user_id_nunique'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf9cf5-c4e6-4d71-a15d-64bea1bf4d55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e09155-217c-4631-b528-ccb7be25ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the created conversion feature and define completed conversion as a click event\n",
    "conversion_completed = feature_creation_df[(feature_creation_df.conversion.notnull()) & \n",
    "                                    (feature_creation_df.event_type == 'ClickEvent')]\n",
    "\n",
    "# calculate conversions, now per user, but can easily be aggregated to session_id instead\n",
    "conversions = conversion_completed.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n",
    "\n",
    "# merge with users, but can easily be done with sessions instead\n",
    "conversion_rate = conversions.merge(users, how='inner', on='time_aggregation', suffixes=('_converting', '_total'))\n",
    "\n",
    "# calculate conversion rate\n",
    "conversion_rate['conversion_rate'] = conversion_rate['user_id_nunique_converting'] / conversion_rate['user_id_nunique_total']\n",
    "\n",
    "conversion_rate.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77c8ee-6038-4b9a-9403-09502899511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize conversion rate\n",
    "fig = px.line(data_frame = conversion_rate[['conversion_rate']].to_pandas())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84339df-e25a-43ff-a527-8ff7aa674128",
   "metadata": {},
   "source": [
    "## Conversion funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa68fc4-e21f-416a-97c2-3070bc7dfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for users that have a conversion event, select their conversion sessions and session_hit_number of the first conversion moment in a session\n",
    "converting_users = conversion_completed.groupby(['user_id', 'session_id']).aggregate({'session_hit_number':'min'})\n",
    "\n",
    "# merge with the df that has all user events in the timeframe\n",
    "converting_users_events = timeframe_df.merge(converting_users, how='inner', on=['user_id', 'session_id'])\n",
    "\n",
    "# select all events that converting users had up to their first conversion moment in the same session\n",
    "converting_users_events = converting_users_events[(converting_users_events['session_hit_number'] <= converting_users_events['session_hit_number_min'])]\n",
    "\n",
    "# filter on only ClickEvent so we focus on user interactions\n",
    "converting_users_events = converting_users_events[(converting_users_events['event_type'] == 'ClickEvent')]\n",
    "\n",
    "# select all unique features used by these users\n",
    "converting_users_features = converting_users_events.groupby(['event_type', 'event_location']).aggregate({'event_id':'nunique'}).sort_values(by='event_location', ascending=True)\n",
    "\n",
    "# now we switch to Pandas, as the dataset is small enough and allows nice visualisation\n",
    "feature_id_pd = converting_users_features.to_pandas().reset_index()\n",
    "\n",
    "# clean-up columns\n",
    "feature_id_pd.drop(columns=['event_id_nunique'], inplace=True)\n",
    "\n",
    "# use the index to give each feature a unique id\n",
    "feature_id_pd['feature_id'] = feature_id_pd.index\n",
    "\n",
    "# create a rolling window that includes the previous event for each row and get it through window_lag()\n",
    "rolling = converting_users_events.sort_values('session_hit_number').groupby('session_id').rolling(2)\n",
    "converting_users_events['prev_event_location'] = rolling.event_location.window_lag()\n",
    "\n",
    "# materizalize the df before we apply an expression on window\n",
    "converting_users_events = converting_users_events.materialize()\n",
    "\n",
    "# group each unique event by previous unique event\n",
    "from_to_events = converting_users_events.groupby(['prev_event_location', 'event_location']).aggregate({'user_id':'nunique'})\n",
    "\n",
    "# now we switch to Pandas, as the dataset is small enough and allows nice visualisation\n",
    "from_to_events_pd = from_to_events.to_pandas().reset_index()\n",
    "\n",
    "# merge with the unique id for each prev_feature\n",
    "sankey_input_pd = from_to_events_pd.merge(feature_id_pd, how='inner', left_on='prev_event_location', right_on='event_location')\n",
    "sankey_input_pd = sankey_input_pd.rename(columns={'event_location_x':'event_location', 'feature_id':'prev_feature_id'})\n",
    "sankey_input_pd = sankey_input_pd.drop(columns={'event_location_y'})\n",
    "\n",
    "# merge with the unique id for each feature\n",
    "sankey_input_pd = sankey_input_pd.merge(feature_id_pd, how='left', left_on='event_location', right_on='event_location')\n",
    "sankey_input_pd = sankey_input_pd.rename(columns={'event_type_x':'event_type'})\n",
    "sankey_input_pd = sankey_input_pd.drop(columns={'event_type_y'})\n",
    "\n",
    "# filter out events where prev_feature and feature are the same and user did not go anywhere new\n",
    "sankey_input_pd = sankey_input_pd[(sankey_input_pd['prev_feature_id'] != sankey_input_pd['feature_id'])]\n",
    "sankey_input_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e73b48-4ab6-4943-b83b-0a41a623c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the sankey\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 50,\n",
    "      thickness = 5,\n",
    "      line = dict(color = \"black\", width = 1),\n",
    "      label = feature_id_pd['event_location'].str.slice(0,30).tolist(),\n",
    "      color = \"blue\",\n",
    "      customdata = (feature_id_pd['event_type'] + ' at ' + feature_id_pd['event_location']).tolist(),\n",
    "      hovertemplate='%{customdata}<br />'+\n",
    "        'unique users: %{value}'  \n",
    "    ),\n",
    "    link = dict(\n",
    "      source = sankey_input_pd['prev_feature_id'].tolist(),\n",
    "      target = sankey_input_pd['feature_id'].tolist(),\n",
    "      value = sankey_input_pd['user_id_nunique'].tolist(),\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Conversion funnel\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c06f91-3fe1-4e25-8706-601e1b223393",
   "metadata": {},
   "source": [
    "## Session duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279e793-e5a4-4a4b-968a-db8c5949f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate duration of each session\n",
    "session_duration = timeframe_df.groupby(['session_id']).aggregate({'moment':['min','max'], 'time_aggregation':'min'})\n",
    "session_duration['session_duration'] = session_duration['moment_max'] - session_duration['moment_min']\n",
    "\n",
    "# check which sessions have duration of zero and filter these out, as they are bounces\n",
    "session_duration = session_duration[(session_duration['session_duration'] > '0')]\n",
    "\n",
    "# rename columns\n",
    "session_duration.rename(columns={'time_aggregation_min':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# calculate average session duration\n",
    "avg_session_duration = session_duration.groupby(['time_aggregation']).aggregate({'session_duration': 'mean'})\n",
    "\n",
    "avg_session_duration.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0f2c5-8e20-4b54-bdfa-dcb920794e5d",
   "metadata": {},
   "source": [
    "## Session duration for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5ab57-27a1-4500-8335-293b94217684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the features we created, select one or more to calculate duration for. In this example we calculate the time\n",
    "# spent in the conversion funnel\n",
    "start_stop = feature_creation_df[feature_creation_df.conversion.notnull()]\n",
    "\n",
    "# get previous moment in the same session, window_lag(n) returns the nth previous value in the partition\n",
    "rolling = start_stop.sort_values('moment').groupby('session_id').rolling(2)\n",
    "start_stop['prev_moment'] = rolling.moment.window_lag()\n",
    "\n",
    "# materizalize the df before we apply an expression on window\n",
    "start_stop = start_stop.materialize()\n",
    "\n",
    "# calculate duration\n",
    "start_stop['duration'] = start_stop.moment - start_stop.prev_moment\n",
    "\n",
    "# calculate average duration per timeframe\n",
    "duration_between_events = start_stop.groupby('time_aggregation').aggregate({'duration':'sum'})\n",
    "\n",
    "duration_between_events.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e283e6-cc77-412d-91ec-9915fcbc7764",
   "metadata": {},
   "source": [
    "## Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069417b1-e554-4e83-9dbb-692258a547eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all sorted time aggregations in the timeframe \n",
    "time_aggregations = timeframe_df.groupby(['time_aggregation']).aggregate({'user_id':'nunique'}).sort_values(by='time_aggregation', ascending=True)\n",
    "time_aggregations.head()\n",
    "\n",
    "# switch to Pandas as the dataset is small enough reset the index, use that to number each cohort\n",
    "time_cohorts = time_aggregations.to_pandas().reset_index()\n",
    "time_cohorts['cohort_id'] = time_cohorts.index\n",
    "time_cohorts.drop(columns=['user_id_nunique'], inplace=True)\n",
    "\n",
    "# select all active moments for each user\n",
    "user_moments = timeframe_df.groupby(['user_id', 'time_aggregation']).aggregate({'moment':'count'})\n",
    "\n",
    "# merge with first seen df\n",
    "user_activity = user_moments.merge(user_first_seen, how='inner', on='user_id')\n",
    "\n",
    "# clean-up and rename columns\n",
    "user_activity.rename(columns={'time_aggregation_min':'new_user_cohort'}, inplace=True)\n",
    "user_activity.drop(columns=['moment_count'], inplace=True)\n",
    "\n",
    "# limit new users to the selected timeframe\n",
    "timeframe_start = timeframe_df['time_aggregation'].min()\n",
    "user_activity = user_activity[(user_activity['new_user_cohort'] >= timeframe_start)]\n",
    "\n",
    "# for each new_user_cohort count how many users get back per timeframe\n",
    "retention_input = user_activity.groupby(['new_user_cohort', 'time_aggregation']).aggregate({'user_id':'nunique'})\n",
    "\n",
    "# add the size of each new user cohort\n",
    "cohorts = retention_input.merge(new_users, how='inner', left_on='new_user_cohort', right_on='time_aggregation_min', suffixes=('_active', '_cohort'))\n",
    "\n",
    "# calculate classic retention (so not rolling retention, where users are required to be active each timeframe)\n",
    "cohorts['retention'] = cohorts['user_id_nunique_active'] / cohorts['user_id_nunique_cohort']\n",
    "\n",
    "# now we switch to Pandas, as the dataset is small enough and allows nice visualisation\n",
    "cohorts_pd = cohorts.to_pandas().reset_index()\n",
    "\n",
    "# merge with cohorts to lookup the id for each new user cohort\n",
    "cohorts_pd = cohorts_pd.merge(time_cohorts, how='inner', left_on='new_user_cohort', right_on='time_aggregation')\n",
    "cohorts_pd.drop(columns=['time_aggregation_y'], inplace=True)\n",
    "cohorts_pd.rename(columns={'cohort_id':'new_user_cohort_id', 'time_aggregation_x':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# merge with cohorts to lookup the id for each active user cohort\n",
    "cohorts_pd = cohorts_pd.merge(time_cohorts, how='inner', on='time_aggregation')\n",
    "cohorts_pd.rename(columns={'cohort_id':'active_user_cohort_id', 'time_aggregation_x':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# number the cohort in which users were active vs their new user cohort\n",
    "cohorts_pd['active_in_timeframe'] = cohorts_pd.active_user_cohort_id - cohorts_pd.new_user_cohort_id\n",
    "\n",
    "# create typical retention matrix\n",
    "cohorts_pd.pivot('new_user_cohort', 'active_in_timeframe', 'retention').replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b83230-19d7-48f9-8230-5f9540db8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove timeframe 0 where the new users are all there, for better visualisation\n",
    "cohorts_pd.drop(cohorts_pd[cohorts_pd.active_in_timeframe == 0].index, inplace=True)\n",
    "\n",
    "# create retention matrix\n",
    "retention_pd = cohorts_pd.pivot('new_user_cohort', 'active_in_timeframe', 'retention').replace(np.nan, 0)\n",
    "\n",
    "# visualize heatmap\n",
    "plt.figure(figsize=(15,10))\n",
    "fmt = lambda x,pos: '{:.0%}'.format(x)\n",
    "retention_heatmap = sns.heatmap(retention_pd, center=1, linewidths=1, square=True, annot=True, fmt=\".0%\", cbar_kws={'format': FuncFormatter(fmt)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdb781-d188-4f7c-b85d-d96262ff7e69",
   "metadata": {},
   "source": [
    "## Bounce rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81a91-e85c-4f96-b534-dd39fdacf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather sessions, hits per timeframe\n",
    "hits_sessions = timeframe_df[['time_aggregation', 'session_id', 'session_hit_number']]\n",
    "\n",
    "# calculate hits per session\n",
    "hits_per_session = hits_sessions.groupby(['time_aggregation', 'session_id']).aggregate({'session_hit_number':'nunique'})\n",
    "\n",
    "# select sessions with only one hit\n",
    "hit_selector = (hits_per_session['session_hit_number_nunique'] == 1)\n",
    "single_hit_sessions = hits_per_session[hit_selector]\n",
    "\n",
    "# count these single hit sessions per timeframe\n",
    "bounced_sessions = single_hit_sessions.groupby('time_aggregation').aggregate({'session_id':'nunique'})\n",
    "\n",
    "# merge with total sessions\n",
    "bounce_rate = bounced_sessions.merge(sessions, how='inner', on='time_aggregation', suffixes=('_bounce', '_total'))\n",
    "\n",
    "# calculate bounce rate\n",
    "bounce_rate['bounce_rate'] = bounce_rate['session_id_nunique_bounce'] / bounce_rate['session_id_nunique_total']\n",
    "\n",
    "# clean-up columns\n",
    "bounce_rate.drop(columns=['session_id_nunique_bounce', 'session_id_nunique_total'], inplace=True)\n",
    "\n",
    "bounce_rate.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974447a7-774a-4713-bb22-61c6b41cd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize bounce rate\n",
    "fig = px.line(data_frame = bounce_rate[['bounce_rate']].to_pandas())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93759009-0366-446a-b759-3f6d5a9a8101",
   "metadata": {},
   "source": [
    "## User agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27cc99-43d2-42cc-8c7e-6273fa27e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to df with the user_agent from the global contexts, using gc function\n",
    "timeframe_df['user_agent'] = timeframe_df.global_contexts.gc.user_agent\n",
    "\n",
    "# gather overall basic stats grouped per user_agent\n",
    "user_agent_counts = timeframe_df.groupby(['time_aggregation', 'user_agent']).aggregate({'user_id':'nunique', 'session_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per user_agent\n",
    "user_agent_counts['total_users'] = total_users\n",
    "\n",
    "# calculate share per user_agent\n",
    "user_agent_counts['share_of_users'] = user_agent_counts['user_id_nunique'] / user_agent_counts['total_users']\n",
    "\n",
    "# clean-up colums\n",
    "user_agent_counts.drop(columns=['total_users'], inplace=True)\n",
    "\n",
    "user_agent_counts.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea590d1-973d-4a3a-a669-eb1e0781f2e6",
   "metadata": {},
   "source": [
    "## Referer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256be167-341e-41dc-ba99-9b42abf22388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to dataframe with the referer from the global contexts, using gc function\n",
    "timeframe_df['referer'] = timeframe_df.global_contexts.gc.get_from_context_with_type_series(type='HttpContext', key='referer')\n",
    "\n",
    "# gather overall basic stats grouped per referer\n",
    "referer_counts = timeframe_df.groupby(['time_aggregation', 'referer']).aggregate({'user_id':'nunique', 'session_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per referer\n",
    "referer_counts['total_users'] = total_users\n",
    "\n",
    "# calculate share per referer\n",
    "referer_counts['share_of_users'] = referer_counts['user_id_nunique'] / referer_counts['total_users']\n",
    "\n",
    "# clean-up colums\n",
    "referer_counts.drop(columns=['total_users'], inplace=True)\n",
    "\n",
    "referer_counts.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd5068-0a4e-4bfb-9e4a-7ba1cfda81bc",
   "metadata": {},
   "source": [
    "## User timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8928f24-9f52-4f44-871b-69fa9628b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the spefic user we want to replay\n",
    "user_selector = (timeframe_df['user_id'].astype('string') == 'fe2657f1-a08c-4e33-b762-441c2f52855c')\n",
    "\n",
    "# create df with only this user's events\n",
    "selected_user_df = timeframe_df[user_selector]\n",
    "\n",
    "# timeline of this user's events\n",
    "user_timeline = selected_user_df[['moment','event_type', 'event_location', 'user_agent', 'referer']]\n",
    "\n",
    "user_timeline.sort_values(by='moment', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59aba6a-6bdb-4a2c-831e-a3655d597a88",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e3f77-417b-4e77-8157-81bdc93fc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total sessions per user\n",
    "total_sessions_user = timeframe_df.groupby(['user_id']).aggregate({'session_id':'nunique'})\n",
    "\n",
    "# calculate frequency\n",
    "frequency = total_sessions_user.groupby(['session_id_nunique']).aggregate({'user_id':'nunique'})\n",
    "\n",
    "# add total users and calculate share per number of sessions\n",
    "frequency['share_of_users'] = frequency['user_id_nunique'] / total_users\n",
    "\n",
    "frequency.sort_values(by='session_id_nunique', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f24409-afd6-4b80-ba15-b787cb219fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize frequency\n",
    "fig = px.bar(data_frame = frequency[['share_of_users']].to_pandas())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3f8b0-8631-4f27-b09d-253dffd3ecd8",
   "metadata": {},
   "source": [
    "## Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dd9c9-e174-465c-8bf8-02515e092038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of active days per user\n",
    "user_active_check = timeframe_df.groupby(['user_id']).aggregate({'day':'nunique'})\n",
    "\n",
    "# select all users that had more than one active day\n",
    "user_active_check = user_active_check[(user_active_check['day_nunique'] > 1)]\n",
    "\n",
    "# select all active days for each user\n",
    "user_days = timeframe_df.groupby(['user_id', 'day']).aggregate({'time_aggregation':'min'})\n",
    "\n",
    "# merge with users that have more than one active day\n",
    "user_days = user_days.merge(user_active_check, how='inner', on='user_id')\n",
    "\n",
    "# reset the index so we can use the user_id & day columns\n",
    "user_days = user_days.reset_index()\n",
    "\n",
    "# get previous (because of the sorting) day for each user\n",
    "rolling = user_days.sort_values('day').groupby(['user_id']).rolling(2)\n",
    "user_days['prev_day'] = rolling.day.window_lag()\n",
    "\n",
    "# materizalize the df before we apply an expression on window\n",
    "user_days = user_days.materialize()\n",
    "\n",
    "# calculate the number of days between an active day and prev_day\n",
    "user_days['recency'] = user_days['day'] - user_days['prev_day']\n",
    "\n",
    "# rename columns\n",
    "user_days.rename(columns={'time_aggregation_min':'time_aggregation'}, inplace=True)\n",
    "\n",
    "# calculate the recency per time_aggregation\n",
    "recency = user_days.groupby(['time_aggregation']).aggregate({'recency':'mean','user_id':'nunique'})\n",
    "\n",
    "recency.sort_values(by='time_aggregation', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b98525-b831-4747-8085-4b883ab76f7e",
   "metadata": {},
   "source": [
    "## Get metrics to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2cba1b-4018-4bb3-a014-bb748ac9728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're working on export functionality to dbt, until then, you can use view_sql() to get the SQL that runs on the full dataset for any metric above\n",
    "\n",
    "# as an example, the SQL for the session duration metric\n",
    "print(avg_session_duration.view_sql())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
