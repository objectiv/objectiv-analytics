{
  "metadata": {
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# install micropip and load required packages\nimport micropip\nawait micropip.install('http://localhost:8000/files/obj_pg_wrapper-0.0.1-py3-none-any.whl')\nawait micropip.install('http://localhost:8000/files/buhtuh-0.0.1-py3-none-any.whl')\nawait micropip.install('http://localhost:8000/files/objectiv_buhtuh-0.0.1-py3-none-any.whl')\nawait micropip.install('plotly')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "16c7424d-1901-4747-829e-78401415f75a"
    },
    {
      "cell_type": "code",
      "source": "# import PG wrapper\nfrom obj_pg_wrapper import create_engine\nimport obj_pg_wrapper",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "8c8814c0-02de-4373-987e-90534d5e00be"
    },
    {
      "cell_type": "code",
      "source": "import sys\nsys.modules['sqlalchemy'] = obj_pg_wrapper\nimport datetime\nimport matplotlib.pyplot as plt\n\n# import Objectiv buh_tuh\nfrom buhtuh.pandasql import BuhTuhDataFrame\nfrom objectiv_buhtuh.util import duplo_basic_features",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "8fcf4b82-b22a-4447-a276-df9778d5714e"
    },
    {
      "cell_type": "markdown",
      "source": "## Get website production data",
      "metadata": {},
      "id": "0a8b47e4-e1c6-443f-a164-31078ed3ae8b"
    },
    {
      "cell_type": "code",
      "source": "## get some data, add database and credentials here\nengine = create_engine('http://localhost:5000')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "c1e2a831"
    },
    {
      "cell_type": "code",
      "source": "## production website data (from sessionized_data + features)\nbasic_features = duplo_basic_features()\nfull_df = BuhTuhDataFrame.from_model(engine=engine, model=basic_features, index=['event_id'])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "8310a973"
    },
    {
      "cell_type": "markdown",
      "source": "## Set the timeframe",
      "metadata": {
        "tags": []
      },
      "id": "237a13bf-52e0-4082-a4a2-c382871b1713"
    },
    {
      "cell_type": "code",
      "source": "# set the timeframe for analysis\nselector = (full_df['moment'] >= datetime.date(2021,6,1)) & (full_df['moment'] < datetime.date(2021,10,11))\n\n# create one sampled df with timeframe applied \ntimeframe_df = full_df[selector]\n\ntimeframe_df.sort_values(by='moment', ascending=False).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "3177f1fb-c741-4269-8086-dbab942afbeb"
    },
    {
      "cell_type": "markdown",
      "source": "## Set the time aggregation ",
      "metadata": {},
      "id": "0f1d4954-cabc-44e7-81b8-d18f9d303ed9"
    },
    {
      "cell_type": "code",
      "source": "# choose for which level of time aggregation the rest of the analysis will run\n# supports all Postgres datetime template patterns:\n# https://www.postgresql.org/docs/9.1/functions-formatting.html#FUNCTIONS-FORMATTING-DATETIME-TABLE\n\nagg_level = 'YYYYIW'\n\n# add the time aggregation as new column to the dataframes, so we can group on this later\ntimeframe_df['time_aggregation'] = timeframe_df['moment'].format(agg_level)\nfull_df['time_aggregation'] = full_df['moment'].format(agg_level)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "0a697920-a443-4593-8acd-71c514aa7406"
    },
    {
      "cell_type": "markdown",
      "source": "## Users",
      "metadata": {
        "tags": []
      },
      "id": "0697e0d3-8443-427b-a3a9-09342adf624e"
    },
    {
      "cell_type": "code",
      "source": "# calculate unique users \nusers = timeframe_df.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n\nusers.sort_values(by='time_aggregation', ascending=False).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "77e00ffd-9940-4ff3-b5db-2734730f240e"
    },
    {
      "cell_type": "code",
      "source": "users.sort_values(by='time_aggregation', ascending=True).head(60).plot(kind='line')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "796082da-6d9b-4754-96c2-5f737183a897"
    },
    {
      "cell_type": "markdown",
      "source": "## Sessions",
      "metadata": {},
      "id": "378f5454-f7c7-409b-a6d0-996e078c851a"
    },
    {
      "cell_type": "code",
      "source": "# calculate unique sessions\nsessions = timeframe_df.groupby('time_aggregation').aggregate({'session_id':'nunique'})\n\nsessions.sort_values(by='time_aggregation', ascending=False).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "71c3c2a3-9616-43d6-b529-acf5f6d210d1"
    },
    {
      "cell_type": "code",
      "source": "# visualize sessions\nsessions.sort_values('time_aggregation', ascending=True).head(60).plot()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "560fc334-17c9-4604-85dd-24fa43abaf07"
    },
    {
      "cell_type": "markdown",
      "source": "## Sessions per user",
      "metadata": {},
      "id": "6888435d-4287-4cdc-8767-985fe2b9c216"
    },
    {
      "cell_type": "code",
      "source": "# merge users and sessions\nusers_sessions = sessions.merge(users, how='inner', on='time_aggregation')\n\n# calculate average sessions per user\nusers_sessions['sessions_per_user_avg'] = users_sessions['session_id_nunique'] / users_sessions['user_id_nunique']\n\n# clean-up columns\ndel(users_sessions['session_id_nunique'])\ndel(users_sessions['user_id_nunique'])\n\nusers_sessions.sort_values('time_aggregation', ascending=False).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "77706040"
    },
    {
      "cell_type": "code",
      "source": "# visualize average sessions per user\nusers_sessions.sort_values(by='time_aggregation', ascending=True).head(60).plot()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "6a7d52d8-08df-4133-a2da-6cedd81f22fd"
    },
    {
      "cell_type": "markdown",
      "source": "## New users",
      "metadata": {},
      "id": "6fbaaf82-ee41-4602-a2cd-f622fa84b0b2"
    },
    {
      "cell_type": "code",
      "source": "# define first seen per user, based on full dataset\nuser_first_seen = full_df.groupby('user_id').aggregate({'time_aggregation':'min'})\n\n# calculate new users for each timeframe\nnew_users = user_first_seen.groupby('time_aggregation_min').aggregate({'user_id':'nunique'})\n\n# merge with total users, to calculate ratio and limit to timerange\nnew_total_users = users.merge(new_users, how='inner', left_on='time_aggregation', right_on='time_aggregation_min', suffixes=('_total', '_new'))\n\n# NOTE: also would be good to delete the index column time_aggregation_min, but we have no function for this yet\n\n# calculate new & returning user share\nnew_total_users['new_user_share'] = new_total_users['user_id_nunique_new'] / new_total_users['user_id_nunique_total']\nnew_total_users['returning_user_share'] = (new_total_users['user_id_nunique_total'] - new_total_users['user_id_nunique_new']) / new_total_users['user_id_nunique_total']\n\nnew_total_users.sort_values(by='time_aggregation', ascending=False).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "d917e1a3-5b16-4365-98e0-ea0040babebe"
    },
    {
      "cell_type": "code",
      "source": "# visualize new users\nnew_total_users[['user_id_nunique_new', 'user_id_nunique_total']].sort_values(by='time_aggregation', ascending=True).head(60).plot()\nplt.show()\n# NOTE: also would be good to delete the index column time_aggregation_min, but we have no function for this yet, not the x-axis is showing twice",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "eca9d062-dd61-49c2-b669-bf0f45c68451"
    },
    {
      "cell_type": "code",
      "source": "# visualize returning users\nnew_total_users[['returning_user_share']].sort_values(by='time_aggregation', ascending=True).head(60).plot()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "be740b11-d552-4501-b8b3-48cb077f8470"
    },
    {
      "cell_type": "markdown",
      "source": "## Frequency",
      "metadata": {},
      "id": "14b2a479-bd03-4cba-be97-83ccfbf38337"
    },
    {
      "cell_type": "code",
      "source": "# calculate total users\n# NOTE; this is not possible in pandas, and therefore not super intuitive. timeframe_df['user_id'].nunique() should be working, on the list\ntotal_users = timeframe_df.groupby()['user_id'].nunique()\n\n# number of total sessions per user\ntotal_sessions_user = timeframe_df.groupby(['user_id']).aggregate({'session_id':'nunique'})\n\n# calculate frequency\nfrequency = total_sessions_user.groupby(['session_id_nunique']).aggregate({'user_id':'nunique'})\n\n# add total users and calculate share per number of sessions\nfrequency['share_of_users'] = frequency['user_id_nunique'] / total_users['user_id_nunique'][1]\n\nfrequency.sort_values(by='session_id_nunique', ascending=True).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "2d8fc165-5b6a-4df4-839e-53b6df4084f9"
    },
    {
      "cell_type": "code",
      "source": "# visualize frequency\nfrequency[['share_of_users']].sort_values(by='session_id_nunique', ascending=True).head(60).plot(kind='bar')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "771461f0-9cc6-482e-ada3-76a4da4152de"
    },
    {
      "cell_type": "markdown",
      "source": "## Events",
      "metadata": {},
      "id": "046eb7e6-fba5-4048-ba17-8cff59d92b38"
    },
    {
      "cell_type": "code",
      "source": "# number of total user and hits per feature\nusers_per_event = timeframe_df.groupby(['time_aggregation', 'feature']).aggregate({'user_id':'nunique','session_hit_number':'count'})\n\nusers_per_event.sort_values(by=['time_aggregation', 'user_id_nunique'], ascending=False).head()\n\n# TODO: \n# 1) add feature aggregation magic here, so we make the features show-off what we can do much more \n# 2) add location stack, showing the power of this very soon in the demo's",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "24c743b4-dae4-41b9-a4d5-15a87c3b295c"
    },
    {
      "cell_type": "markdown",
      "source": "## Conversion",
      "metadata": {
        "tags": []
      },
      "id": "9acf9cf5-c4e6-4d71-a15d-64bea1bf4d55"
    },
    {
      "cell_type": "code",
      "source": "# TODO: \n# We can do much better here once we integrate feature selection & aggregation\n\n# NOTE: WE NEED TO UPDATE THIS ONCE THE FIRST NEW EVENT FORMAT DATA COMES IN FOR A CONVERSION\n# set the goal event that you define as conversion, using our subcribe-to-mailing\nconv_selector = (timeframe_df['feature'] == '(WebDocumentContext,#document),(InputContext,keep-me-posted-input),(ButtonContext,subscribe)')\n\n# create df with only conversion events\nconversions_df = timeframe_df[conv_selector]\n\n# calculate conversions, now per user, but can easily be aggregated to session_id instead\nconversions = conversions_df.groupby('time_aggregation').aggregate({'user_id':'nunique'})\n\n# merge with users, but can easily be done with sessions instead\nconversion_rate = conversions.merge(users, how='inner', on='time_aggregation', suffixes=('_converting', '_total'))\n\n# calculate conversion rate\nconversion_rate['conversion_rate'] = conversion_rate['user_id_nunique_converting'] / conversion_rate['user_id_nunique_total']\n\nconversion_rate.sort_values(by='time_aggregation', ascending=False).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "c8e09155-217c-4631-b528-ccb7be25ac43"
    },
    {
      "cell_type": "code",
      "source": "# visualize conversion rate\nconversion_rate[['conversion_rate']].sort_values(by='time_aggregation', ascending=True).head(60).plot()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "91715bec-e0c2-4abc-b049-0b7b590348ee"
    },
    {
      "cell_type": "markdown",
      "source": "## Bounce rate",
      "metadata": {},
      "id": "e3cdb781-d188-4f7c-b85d-d96262ff7e69"
    },
    {
      "cell_type": "code",
      "source": "# NOTE: we need to limit this to page or screen views, instead of all events. Do this once we have integration feature selection.\n\n# gather sessions, hits per timeframe\nhits_sessions = timeframe_df[['time_aggregation', 'session_id', 'session_hit_number']]\n\n# calculate hits per session\nhits_per_session = hits_sessions.groupby(['time_aggregation', 'session_id']).aggregate({'session_hit_number':'nunique'})\n\n# select sessions with only one hit\nhit_selector = (hits_per_session['session_hit_number_nunique'] == 1)\nsingle_hit_sessions = hits_per_session[hit_selector].to_frame()\n\n# count these single hit sessions per timeframe\nbounced_sessions = single_hit_sessions.groupby('time_aggregation').aggregate({'session_id':'nunique'})\n\n# merge with total sessions\nbounce_rate = bounced_sessions.merge(sessions, how='inner', on='time_aggregation', suffixes=('_bounce', '_total'))\n\n# calculate bounce rate\nbounce_rate['bounce_rate'] = bounce_rate['session_id_nunique_bounce'] / bounce_rate['session_id_nunique_total']\n\nbounce_rate.sort_values(by='time_aggregation', ascending=False).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "98f81a91-e85c-4f96-b534-dd39fdacf55e"
    },
    {
      "cell_type": "code",
      "source": "# visualize bounce rate\nbounce_rate[['bounce_rate']].sort_values(by='time_aggregation', ascending=True).head(60).plot()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "6d126de4-cb9d-4f94-8d54-4074721385db"
    },
    {
      "cell_type": "markdown",
      "source": "## Session duration",
      "metadata": {},
      "id": "26c06f91-3fe1-4e25-8706-601e1b223393"
    },
    {
      "cell_type": "code",
      "source": "# calculate duration of each session\n# NOTE: we want this to work, but that is a bug, on the list:\n# session_duration = timeframe_df.groupby(['session_id']).aggregate({'moment':['min','max'],'time_aggregation':'min'})\n\nsession_duration = timeframe_df.groupby(['session_id']).aggregate(['moment', 'moment', 'time_aggregation'], ['min', 'max', 'min'])\n\nsession_duration['session_duration'] = session_duration['moment_max'] - session_duration['moment_min']\n\n# check which sessions have duration of zero\n# NOTE: not very intuitive. on the list to improve\nsession_duration['session_duration_zero'] = session_duration['session_duration'] == '0'\n\n# calculate average session duration\navg_session_duration = session_duration.groupby(['time_aggregation_min', 'session_duration_zero']).aggregate(['session_duration', 'session_id'],['average', 'count'])\n\n# merge with total sessions and calculate share\nduration_breakdown = avg_session_duration.merge(sessions, how='inner', left_on='time_aggregation_min', right_on='time_aggregation')\n\n# clean-up and rename columns\nduration_breakdown['share_of_sessions'] = duration_breakdown['session_id_count'] / duration_breakdown['session_id_nunique']\ndel(duration_breakdown['session_id_nunique'])\n\nduration_breakdown.sort_values(by='time_aggregation_min', ascending=False).head(6)\n\n# NOTE: also would be good to delete the index column time_aggregation, but we have no function for this yet",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "2279e793-e5a4-4a4b-968a-db8c5949f0ff"
    },
    {
      "cell_type": "markdown",
      "source": "## Session duration between events",
      "metadata": {},
      "id": "a2f0f2c5-8e20-4b54-bdfa-dcb920794e5d"
    },
    {
      "cell_type": "code",
      "source": "# define the start and stop events to measure the duration in between\nstart_event = '(WebDocumentContext,#document)'\nstop_event = '(WebDocumentContext,#document),(SectionContext,footer)'\n\n# filter on only these events\nstart_stop = timeframe_df[(timeframe_df.feature == start_event) | (timeframe_df.feature == stop_event)]\n\n# get previous (because of the sorting) event for stop event _in the same session, window_lag(n) returns the nth previous value in the partition\nwindow = start_stop.sort_values('moment').window('session_id')\nstart_stop['prev_event'] = start_stop.feature.window_lag(window)\nstart_stop['prev_moment'] = start_stop.moment.window_lag(window)\n\n# create a copy of this df with as base_node the current df's state\n# note: this is a temp fix until we automatically create a new node\nstart_stop = start_stop.get_df_materialized_model()\n\n# filter: for each stop event, select the closest preceeding start event\ncomplete = start_stop[(start_stop.feature == stop_event) & (start_stop.prev_event == start_event)]\n\n# calculate duration\ncomplete['duration'] = complete.moment - complete.prev_moment\n\n# calculate average duration per timeframe\nduration_between_events = complete.groupby('time_aggregation').aggregate({'duration':'average'})\n\nduration_between_events.sort_values(by='time_aggregation', ascending=False).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "36bb0024-91db-4a8b-a045-21934c117020"
    },
    {
      "cell_type": "markdown",
      "source": "## Retention",
      "metadata": {},
      "id": "76e283e6-cc77-412d-91ec-9915fcbc7764"
    },
    {
      "cell_type": "code",
      "source": "# select all active moments for each user\nuser_moments = timeframe_df.groupby(['user_id', 'time_aggregation']).aggregate({'moment':'count'})\n\n# merge with first seen df\nuser_activity = user_moments.merge(user_first_seen, how='inner', on='user_id')\n\n# clean-up and rename columns\nuser_activity['new_user_cohort'] = user_activity['time_aggregation_min']\ndel(user_activity['time_aggregation_min'])\ndel(user_activity['moment_count'])  \n\n# for each new_user_cohort count how many users get back per timeframe\nretention_input = user_activity.groupby(['new_user_cohort', 'time_aggregation']).aggregate({'user_id':'nunique'})\n\n# add the size of each new user cohort\ncohorts = retention_input.merge(new_users, how='inner', left_on='new_user_cohort', right_on='time_aggregation_min', suffixes=('_active', '_cohort'))\n\n# NOTE: after we can rename/delete an index, remove the time_aggregation_min column here, it's duplicate\n\n# calculate classic retention (so not rolling retention, where users are required to be active each timeframe)\ncohorts['retention'] = cohorts['user_id_nunique_active'] / cohorts['user_id_nunique_cohort']\n\n# now switch to Pandas, as the dataset is small enough\ncohorts_df = cohorts.to_df().reset_index()\n\n# create typical retention matrix\ncohorts_df = cohorts_df.astype({'new_user_cohort': 'int', 'time_aggregation': 'int'})\ncohorts_df['active_in_timeframe'] = cohorts_df.time_aggregation - cohorts_df.new_user_cohort\ncohorts_df.pivot('new_user_cohort', 'active_in_timeframe', 'retention')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "9274e78b-c981-49f4-b30d-1a5729dc6268"
    },
    {
      "cell_type": "markdown",
      "source": "## User timeline",
      "metadata": {},
      "id": "3d2c66a2-97ee-4888-83d2-122547bf53c6"
    },
    {
      "cell_type": "code",
      "source": "# show the timeline of an indivual user's events\n# NOTE: we can make this better with feature selection & aggregation\n\n# select the spefic user we want to replay\n# NOTE: .astype('string') is more something buhtuh should handle, on list\nuser_selector = (timeframe_df['user_id'].astype('string') == '320db8ee-847c-424b-8291-c65d021575aa')\n\n# create df with only this user's events\nselected_user_df = timeframe_df[user_selector]\n\n# NOTE: we can apply feature selection and maybe sankey visual here\n# timeline of this user's events\nuser_timeline = selected_user_df[['moment','feature']]\n\nuser_timeline.sort_values(by='moment', ascending=True).head(30)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "4c13effa-fbc1-42fb-8faf-85c40948602b"
    },
    {
      "cell_type": "markdown",
      "source": "# TODO",
      "metadata": {},
      "id": "39f25752-6cbe-4119-ac7e-79d4c095e793"
    },
    {
      "cell_type": "markdown",
      "source": "## WIP Recency",
      "metadata": {},
      "id": "95769feb-fc26-4aa1-9e12-205aa3ed7fc8"
    },
    {
      "cell_type": "code",
      "source": "# select all active days for each user\nuser_days = timeframe_df.groupby(['user_id', 'day'])['session_id'].nunique()\n\nuser_days['day_copy'] = user_days.index['day']\n\n# get previous (because of the sorting) day for each user\n# window = user_days.sort_values('day').window('user_id')\n# user_days['prev_day'] = user_days.day.window_lag(window)\n\n#user_days.head()\n# create a copy of this df with as base_node the current df's state\n# note: this is a temp fix until we automatically create a new node\n#start_stop = start_stop.get_df_materialized_model()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "73d3fef3-e4d7-4adf-a8c3-ca4c4d789606"
    },
    {
      "cell_type": "code",
      "source": "# below parts first require some next steps in dub_buh_tuh",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "0a504f06-ce9b-44d0-b1e1-bc4047a51843"
    },
    {
      "cell_type": "markdown",
      "source": "## WIP Stack",
      "metadata": {},
      "id": "bbf27126-3491-4468-ac8f-5b2df69d28c7"
    },
    {
      "cell_type": "code",
      "source": "# timeframe_df.global_contexts.json.get_value('ApplicationContext').head()\n\ntimeframe_df.global_contexts.json[0].head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "b6389f4b-0bb6-4692-b865-3f64aa0ef26f"
    },
    {
      "cell_type": "markdown",
      "source": "## Conversion funnel",
      "metadata": {},
      "id": "c84339df-e25a-43ff-a527-8ff7aa674128"
    },
    {
      "cell_type": "code",
      "source": "# TODO\n# Self-merge is giving not the ouput we expect. \n# Without that, we can not create a sankey that looks like a familiar funnel. \n# See example here https://gitlab.com/newrelity/objectiv-taxonomy-prototypes/-/blob/web-analytics/data-science/issue_example_self_merge.ipynb\n\n# showing the sequence of events for converting users\n\n# resuse the df with only conversion events, select the users and their conversion moment\nconverting_users = conversions_df['user_id', 'moment']\n\n# for now, we focus on the first conversion event. Later it is nice to also make it possible to see events between first and 2nd conversion, and so on.\nconverting_users = converting_users.groupby(['user_id'])['moment'].min()\nconverting_users['first_conversion_moment'] = converting_users['moment_min']\ndel(converting_users['moment_min'])\n\n# merge with the df that has all user events in the timeframe\nconverting_users_events = timeframe_df.merge(converting_users, [('user_id', 'user_id')])\n\n# select all events that converting users had up to their first conversion moment\nevent_selector = (converting_users_events['moment'] <= converting_users_events['first_conversion_moment'])\npre_conversion_events = converting_users_events[event_selector]\n\n# create pairs of from-to events based on session hit number\nevent_sequence = pre_conversion_events['session_id', 'session_hit_number', 'feature']\n\nevent_pairs = event_sequence.merge(event_sequence, [('session_id')])\n\nevent_pairs.head(50)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "4d2ea3f2-a2b0-4701-80fe-3cce1e54b69b"
    },
    {
      "cell_type": "code",
      "source": "df_sank = pd.read_csv('buh.csv')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "9c8ae37e-d3a9-4ff4-a812-58344f07c7f5"
    },
    {
      "cell_type": "code",
      "source": "categories = set(df_sank['source']).union(set(df_sank['target']))\ndf_sank['source'] = pd.Categorical(df_sank['source'], categories=categories)\ndf_sank['target'] = pd.Categorical(df_sank['target'], categories=categories)\n\ntext_in_title = str('title')\nnode = dict(\n      pad=15,\n      thickness=20,\n      line=dict(color=\"black\", width=0.5),\n      label=df_sank.source.cat.categories,\n      color='blue'\n    )\nlink = pd.concat([df_sank[['source', 'target']].apply(lambda x: x.cat.codes), df_sank['value']], axis=1).to_dict('list')\nfig = go.Figure(go.Sankey(arrangement=\"fixed\", link=link, node=node), {'clickmode': 'event+select'})\nfig.update_layout(title_text=text_in_title, font_size=10)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "4f7547db-3fad-412c-b566-01f5582dccdf"
    },
    {
      "cell_type": "markdown",
      "source": "## Events flow",
      "metadata": {},
      "id": "19ec41a8-09bc-4922-b12f-83dd1725bf67"
    },
    {
      "cell_type": "code",
      "source": "# events per session hit number\nevents_per_hit_number = timeframe_df[selector].groupby(['session_hit_number', 'feature'])['session_id'].nunique()\n\nevents_per_hit_number.sort_values({'session_hit_number':True}).head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "106f06c4-ac0f-4c6b-889b-918cba8be266"
    },
    {
      "cell_type": "markdown",
      "source": "## Traffic source",
      "metadata": {},
      "id": "6589de70-d5d2-4b1b-8529-ed12c0d3e898"
    },
    {
      "cell_type": "code",
      "source": "# TODO\n# For Traffic Source, Geo and Device metrics, we would need to get source/geo/device data from GlobalContext in a easy way.\n# We can then also blend it in all metrics above as slicing option.",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "8d42d550-625b-451f-8cb4-fed192483469"
    },
    {
      "cell_type": "markdown",
      "source": "## Geo ",
      "metadata": {
        "tags": []
      },
      "id": "e56cb88f-b2df-44e2-80ba-c1c384978689"
    },
    {
      "cell_type": "markdown",
      "source": "## Devices",
      "metadata": {},
      "id": "21a12b72-3872-488c-934d-eed3f262560e"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "3ef810cd-06ff-4b77-8804-adc305291cdc"
    }
  ]
}