{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dbt import tracking\n",
    "from dbt.adapters.factory import get_adapter\n",
    "from dbt.tracking import User\n",
    "\n",
    "from dbt_utils import get_sql, get_compile_task, get_sql2\n",
    "tracking.active_user = User('')\n",
    "tracking.active_user.do_not_track = True\n",
    "\n",
    "#sql = get_sql(model='stats_per_feature_day', vars={'feature_table': 'basic_features'})\n",
    "#sql2 = get_sql2(model='stats_per_feature_day', vars={'feature_table': 'basic_features'})\n",
    "ctask = get_compile_task(model='stats_per_feature_day', vars={'feature_table': 'basic_features'})\n",
    "ctask.config.target_path\n",
    "#ctask.graph is None\n",
    "ctask.manifest is None\n",
    "#graph_queue = ctask.get_graph_queue()\n",
    "#print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctask.load_manifest()\n",
    "ctask.compile_manifest()\n",
    "manifest = ctask.manifest\n",
    "#ctask.graph\n",
    "\n",
    "# graph = ctask.graph.graph\n",
    "# node_id = 'model.objectiv.stats_per_feature_day'\n",
    "# z = manifest.nodes['model.objectiv.stats_per_feature_day']\n",
    "# #graph.manifest.nodes\n",
    "# #x= graph.pred[node_id]\n",
    "# adapter = get_adapter(ctask.config)\n",
    "# compiler = adapter.get_compiler()\n",
    "# selector = ctask.get_node_selector()\n",
    "# spec = ctask.get_selection_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "CompiledModelNode(raw_sql='--\\n-- Per feature and day the number of unique users, sessions and events\\n--\\nselect\\n       f.feature_pretty_name,\\n       sd.day,\\n       count(distinct sd.user_id) as users,\\n       count(distinct sd.session_id) as sessions,\\n       count(*) as events\\nfrom {{ ref(\\'sessionized_data\\') }} as sd\\ninner join {{ ref( var(\"feature_table\") | as_text ) }} as f using (feature_hash)\\ngroup by f.feature_pretty_name, sd.day', compiled=True, database='objectiv', schema='public', fqn=['objectiv', 'stats', 'stats_per_feature_day'], unique_id='model.objectiv.stats_per_feature_day', package_name='objectiv', root_path='/home/thijs/workspace/git-repos/objectiv-analytics/backend/data_models', path='stats/stats_per_feature_day.sql', original_file_path='models/stats/stats_per_feature_day.sql', name='stats_per_feature_day', resource_type=<NodeType.Model: 'model'>, alias='stats_per_feature_day', checksum=FileHash(name='sha256', checksum='d8b51c9a6a3051ff74817f4d2d3fbfdf0a3f693f4dcfd494e4d5cf24e43328ee'), config=NodeConfig(_extra={'feature_creation': None, 'data_queries': None}, enabled=True, materialized='view', persist_docs={}, post_hook=[], pre_hook=[], vars={}, quoting={}, column_types={}, alias=None, schema=None, database=None, tags=[], full_refresh=None), tags=[], refs=[['sessionized_data'], ['basic_features']], sources=[], depends_on=DependsOn(macros=[], nodes=['model.objectiv.sessionized_data', 'model.objectiv.basic_features']), description='', columns={}, meta={}, docs=Docs(show=True), patch_path=None, build_path=None, deferred=False, unrendered_config={'materialized': 'view', 'feature_creation': None, 'data_queries': None}, compiled_sql='with __dbt__CTE__extracted_contexts as (\\nSELECT *,\\n        value->>\\'event\\' AS event,\\n        JSON_EXTRACT_PATH(value, \\'global_contexts\\') AS global_contexts,\\n        JSON_EXTRACT_PATH(value, \\'location_stack\\') AS location_stack,\\n        JSON_EXTRACT_PATH(value, \\'time\\') AS time,\\n        JSON_EXTRACT_PATH(value, \\'events\\') AS events\\n FROM \"objectiv\".\"public\".\"data\"\\n),  __dbt__CTE__hashed_features as (\\nWITH selected_stacks AS\\n(\\nSELECT   event_id,\\n         Array_to_string(Array_agg(Cast(x AS TEXT)),\\',\\') AS stack_selection,\\n         Array_to_json(Array_agg(Row_to_json(x))) as selected_stack_location\\nFROM     __dbt__CTE__extracted_contexts,\\n         json_to_recordset(location_stack) AS x(_context_type text,id text)\\nGROUP BY event_id\\nORDER BY event_id\\n)\\nSELECT *,\\n       md5(concat(stack_selection,event)) as feature_hash\\nFROM __dbt__CTE__extracted_contexts\\nJOIN selected_stacks USING (event_id)\\n),  __dbt__CTE__sessionized_data as (\\nwith session_starts as (\\n    select\\n        cookie_id as cookie_id,\\n        event_id as event_id,\\n        coalesce(\\n            extract(\\n                epoch from (moment - lag(moment, 1) over (partition by cookie_id order by moment, event_id))\\n            ) > 5,\\n            true\\n        ) as is_start_of_session,\\n        moment as moment\\n    from __dbt__CTE__hashed_features\\n),\\nsession_id_and_start as (\\n    select\\n           -- TODO: do something smart so this can scale.\\n           -- currently we always have to query all data. We want to have consistent session_ids, but don\\'t\\n           -- want to calculate them from scratch every time.\\n           -- uuid_generate_v1() as session_id,\\n           row_number() over (order by moment asc) as session_id,\\n           cookie_id,\\n           event_id,\\n           moment as moment\\n    from session_starts\\n    where is_start_of_session\\n)\\nselect\\n        s.session_id as session_id,\\n        row_number() over (partition by s.session_id order by d.moment, d.event_id asc) as session_hit_number,\\n        d.cookie_id as user_id,\\n        d.*\\nfrom __dbt__CTE__hashed_features as d\\ninner join session_id_and_start as s on s.cookie_id = d.cookie_id and s.moment <= d.moment\\nwhere not exists (\\n    select *\\n    from session_id_and_start as s2\\n    where\\n      -- a session start for the same cookie\\n          s2.cookie_id = d.cookie_id\\n      and s2.moment <= d.moment\\n      -- and that session is closer to pq.moment than the selected session s\\n      and s2.moment > s.moment\\n)\\norder by session_id, moment\\n),  __dbt__CTE__basic_features as (\\nSELECT DISTINCT feature_hash,\\n                stack_selection as feature_name,\\n                \\'Pretty\\' || stack_selection as feature_pretty_name\\n-- TODO, get this from a variable to use dynamic feature selection\\nFROM __dbt__CTE__hashed_features\\n)--\\n-- Per feature and day the number of unique users, sessions and events\\n--\\nselect\\n       f.feature_pretty_name,\\n       sd.day,\\n       count(distinct sd.user_id) as users,\\n       count(distinct sd.session_id) as sessions,\\n       count(*) as events\\nfrom __dbt__CTE__sessionized_data as sd\\ninner join __dbt__CTE__basic_features as f using (feature_hash)\\ngroup by f.feature_pretty_name, sd.day', extra_ctes_injected=True, extra_ctes=[InjectedCTE(id='model.objectiv.extracted_contexts', sql=' __dbt__CTE__extracted_contexts as (\\nSELECT *,\\n        value->>\\'event\\' AS event,\\n        JSON_EXTRACT_PATH(value, \\'global_contexts\\') AS global_contexts,\\n        JSON_EXTRACT_PATH(value, \\'location_stack\\') AS location_stack,\\n        JSON_EXTRACT_PATH(value, \\'time\\') AS time,\\n        JSON_EXTRACT_PATH(value, \\'events\\') AS events\\n FROM \"objectiv\".\"public\".\"data\"\\n)'), InjectedCTE(id='model.objectiv.hashed_features', sql=\" __dbt__CTE__hashed_features as (\\nWITH selected_stacks AS\\n(\\nSELECT   event_id,\\n         Array_to_string(Array_agg(Cast(x AS TEXT)),',') AS stack_selection,\\n         Array_to_json(Array_agg(Row_to_json(x))) as selected_stack_location\\nFROM     __dbt__CTE__extracted_contexts,\\n         json_to_recordset(location_stack) AS x(_context_type text,id text)\\nGROUP BY event_id\\nORDER BY event_id\\n)\\nSELECT *,\\n       md5(concat(stack_selection,event)) as feature_hash\\nFROM __dbt__CTE__extracted_contexts\\nJOIN selected_stacks USING (event_id)\\n)\"), InjectedCTE(id='model.objectiv.sessionized_data', sql=\" __dbt__CTE__sessionized_data as (\\nwith session_starts as (\\n    select\\n        cookie_id as cookie_id,\\n        event_id as event_id,\\n        coalesce(\\n            extract(\\n                epoch from (moment - lag(moment, 1) over (partition by cookie_id order by moment, event_id))\\n            ) > 5,\\n            true\\n        ) as is_start_of_session,\\n        moment as moment\\n    from __dbt__CTE__hashed_features\\n),\\nsession_id_and_start as (\\n    select\\n           -- TODO: do something smart so this can scale.\\n           -- currently we always have to query all data. We want to have consistent session_ids, but don't\\n           -- want to calculate them from scratch every time.\\n           -- uuid_generate_v1() as session_id,\\n           row_number() over (order by moment asc) as session_id,\\n           cookie_id,\\n           event_id,\\n           moment as moment\\n    from session_starts\\n    where is_start_of_session\\n)\\nselect\\n        s.session_id as session_id,\\n        row_number() over (partition by s.session_id order by d.moment, d.event_id asc) as session_hit_number,\\n        d.cookie_id as user_id,\\n        d.*\\nfrom __dbt__CTE__hashed_features as d\\ninner join session_id_and_start as s on s.cookie_id = d.cookie_id and s.moment <= d.moment\\nwhere not exists (\\n    select *\\n    from session_id_and_start as s2\\n    where\\n      -- a session start for the same cookie\\n          s2.cookie_id = d.cookie_id\\n      and s2.moment <= d.moment\\n      -- and that session is closer to pq.moment than the selected session s\\n      and s2.moment > s.moment\\n)\\norder by session_id, moment\\n)\"), InjectedCTE(id='model.objectiv.basic_features', sql=\" __dbt__CTE__basic_features as (\\nSELECT DISTINCT feature_hash,\\n                stack_selection as feature_name,\\n                'Pretty' || stack_selection as feature_pretty_name\\n-- TODO, get this from a variable to use dynamic feature selection\\nFROM __dbt__CTE__hashed_features\\n)\")], relation_name='\"objectiv\".\"public\".\"stats_per_feature_day\"', _pre_injected_sql='--\\n-- Per feature and day the number of unique users, sessions and events\\n--\\nselect\\n       f.feature_pretty_name,\\n       sd.day,\\n       count(distinct sd.user_id) as users,\\n       count(distinct sd.session_id) as sessions,\\n       count(*) as events\\nfrom __dbt__CTE__sessionized_data as sd\\ninner join __dbt__CTE__basic_features as f using (feature_hash)\\ngroup by f.feature_pretty_name, sd.day')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_queue = ctask.get_graph_queue()\n",
    "#graph_queue.get_selected_nodes()\n",
    "# [graph_queue.get().unique_id\n",
    "# ,graph_queue.get().unique_id\n",
    "# ,graph_queue.get().unique_id\n",
    "# ,graph_queue.get().unique_id\n",
    "# ,graph_queue.get().unique_id]\n",
    "\n",
    "result = []\n",
    "while not graph_queue.empty():\n",
    "    node = graph_queue.get()\n",
    "    result.append(node.unique_id)\n",
    "#'model.objectiv.stats_per_feature_day'\n",
    "result\n",
    "y = compiler.compile_node(node=node, manifest=manifest, extra_context=None, write=False)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "POSTGRES_USER =     os.environ.get('POSTGRES_USER',     'objectiv')\n",
    "POSTGRES_PASSWORD = os.environ.get('POSTGRES_PASSWORD', '0bj3ctiv')\n",
    "POSTGRES_DB =       os.environ.get('POSTGRES_DB',       'objectiv')\n",
    "POSTGRES_HOSTNAME = os.environ.get('POSTGRES_HOSTNAME', 'localhost')\n",
    "POSTGRES_PORT =     os.environ.get('POSTGRES_PORT',     '5432')\n",
    "\n",
    "connection = psycopg2.connect(user=POSTGRES_USER,\n",
    "                            password=POSTGRES_PASSWORD,\n",
    "                            host=POSTGRES_HOSTNAME,\n",
    "                            port=POSTGRES_PORT,\n",
    "                            database=POSTGRES_DB)\n",
    "\n",
    "# This file should be mapped to a volume in the docker container,\n",
    "# so changes to this file from within the container can be persisted.\n",
    "\n",
    "with connection:\n",
    "    with connection.cursor(cursor_factory=psycopg2.extras.DictCursor) as cursor:\n",
    "        cursor.execute(sql)\n",
    "        rows = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pretty(NavigationContext,navigation),(OverlayContext,navigation-drawer),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(NavigationContext,navigation),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ButtonContext,prev),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  5],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,5o7WEv5Q5ZE),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  3],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,BeyEGebJ1l4),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  3],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,BodkSiBOw3A),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  3],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,cc91EfoBh8A),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  3],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,dQw4w9WgXcQ),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  3,\n",
       "  4],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,DqZS89jFCFg),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  3,\n",
       "  4],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,eYuUAGXN0KM),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  3,\n",
       "  4],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,yBwD4iYcWC4),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  4],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,yPYZpwSpKmA),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  4],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(ItemContext,ZVshSddRqAc),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  3,\n",
       "  4],\n",
       " ['Pretty(SectionContext,home),(SectionContext,for-you),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,new),(ItemContext,BeyEGebJ1l4),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,new),(ItemContext,BodkSiBOw3A),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,new),(ItemContext,cc91EfoBh8A),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,new),(ItemContext,DqZS89jFCFg),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,new),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(ButtonContext,prev),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(ItemContext,BodkSiBOw3A),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(ItemContext,cc91EfoBh8A),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(ItemContext,DqZS89jFCFg),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  2],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(ItemContext,eYuUAGXN0KM),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(ItemContext,yBwD4iYcWC4),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(ItemContext,yPYZpwSpKmA),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  2],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(ItemContext,ZVshSddRqAc),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  2,\n",
       "  2],\n",
       " ['Pretty(SectionContext,home),(SectionContext,top-10),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,yep),(ItemContext,5o7WEv5Q5ZE),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,yep),(ItemContext,DqZS89jFCFg),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,yep),(ItemContext,eYuUAGXN0KM),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,yep),(ItemContext,yPYZpwSpKmA),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(SectionContext,yep),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(SectionContext,home),(WebDocumentContext,rod-web)',\n",
       "  datetime.date(2021, 5, 6),\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " ['Pretty(WebDocumentContext,rod-web)', datetime.date(2021, 5, 6), 1, 1, 3]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with __dbt__CTE__extracted_contexts as (\\nSELECT *,\\n        value->>\\'event\\' AS event,\\n        JSON_EXTRACT_PATH(value, \\'global_contexts\\') AS global_contexts,\\n        JSON_EXTRACT_PATH(value, \\'location_stack\\') AS location_stack,\\n        JSON_EXTRACT_PATH(value, \\'time\\') AS time,\\n        JSON_EXTRACT_PATH(value, \\'events\\') AS events\\n FROM \"objectiv\".\"public\".\"data\"\\n),  __dbt__CTE__hashed_features as (\\nWITH selected_stacks AS\\n(\\nSELECT   event_id,\\n         Array_to_string(Array_agg(Cast(x AS TEXT)),\\',\\') AS stack_selection,\\n         Array_to_json(Array_agg(Row_to_json(x))) as selected_stack_location\\nFROM     __dbt__CTE__extracted_contexts,\\n         json_to_recordset(location_stack) AS x(_context_type text,id text)\\nGROUP BY event_id\\nORDER BY event_id\\n)\\nSELECT *,\\n       md5(concat(stack_selection,event)) as feature_hash\\nFROM __dbt__CTE__extracted_contexts\\nJOIN selected_stacks USING (event_id)\\n),  __dbt__CTE__sessionized_data as (\\nwith session_starts as (\\n    select\\n        cookie_id as cookie_id,\\n        event_id as event_id,\\n        coalesce(\\n            extract(\\n                epoch from (moment - lag(moment, 1) over (partition by cookie_id order by moment, event_id))\\n            ) > 5,\\n            true\\n        ) as is_start_of_session,\\n        moment as moment\\n    from __dbt__CTE__hashed_features\\n),\\nsession_id_and_start as (\\n    select\\n           -- TODO: do something smart so this can scale.\\n           -- currently we always have to query all data. We want to have consistent session_ids, but don\\'t\\n           -- want to calculate them from scratch every time.\\n           -- uuid_generate_v1() as session_id,\\n           row_number() over (order by moment asc) as session_id,\\n           cookie_id,\\n           event_id,\\n           moment as moment\\n    from session_starts\\n    where is_start_of_session\\n)\\nselect\\n        s.session_id as session_id,\\n        row_number() over (partition by s.session_id order by d.moment, d.event_id asc) as session_hit_number,\\n        d.cookie_id as user_id,\\n        d.*\\nfrom __dbt__CTE__hashed_features as d\\ninner join session_id_and_start as s on s.cookie_id = d.cookie_id and s.moment <= d.moment\\nwhere not exists (\\n    select *\\n    from session_id_and_start as s2\\n    where\\n      -- a session start for the same cookie\\n          s2.cookie_id = d.cookie_id\\n      and s2.moment <= d.moment\\n      -- and that session is closer to pq.moment than the selected session s\\n      and s2.moment > s.moment\\n)\\norder by session_id, moment\\n),  __dbt__CTE__basic_features as (\\nSELECT DISTINCT feature_hash,\\n                stack_selection as feature_name,\\n                \\'Pretty\\' || stack_selection as feature_pretty_name\\n-- TODO, get this from a variable to use dynamic feature selection\\nFROM __dbt__CTE__hashed_features\\n)--\\n-- Per feature and day the number of unique users, sessions and events\\n--\\nselect\\n       f.feature_pretty_name,\\n       sd.day,\\n       count(distinct sd.user_id) as users,\\n       count(distinct sd.session_id) as sessions,\\n       count(*) as events\\nfrom __dbt__CTE__sessionized_data as sd\\ninner join __dbt__CTE__basic_features as f using (feature_hash)\\ngroup by f.feature_pretty_name, sd.day'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}